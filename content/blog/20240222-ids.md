+++
date = "2024-02-20T21:13:54-07:00"
title = "The importance of Data Schemas and IDs"
tags = [ ""
]
summary = ""
author = "Chris Holmes"
author_url = "https://beta.source.coop/cholmes"
author_title = "Radiant Earth Technical Fellow"
+++

So I have a large backlog of posts I'd like to write, mostly going into a lot more depth on the ideas I presented in my FOSS4G-NA talk.

Please do watch the talk (or feel free to just read the speaker notes in [the slide deck](https://docs.google.com/presentation/d/15wi7XAWm2gP-7Gg0hjFeRaIxS18M-mG6DVh1F8j-lgM/edit) if, like me, you prefer to read the information) to get the general thrust of my thinking, as I don't yet have the blog posts to refer to. I had to cut a lot of detail to get the talk to 25 minutes, and in general plan to expand many of the sections (where SDI's went wrong, PMTiles, Table Formats, Flipping SDI on its head, Why CNSDI is better, etc.) into full blog posts. But for this post I want to dig into something I mentioned in the talk &ndash; the importance of data schemas and ID's.

I've spent the last few years on data formats (COG, STAC and GeoParquet), and making 'Cloud Native Geospatial' a thing. But the goal is not to make more formats! GeoParquet is definitely going to be the last format I work on, since with COPC, Zarr & PMTiles I believe we've got all the fundamental data types covered. To be clear this is not to say I think those are going to be the formats that live forever &ndash; I welcome more cloud-native format innovation, I'm just not going to be the one leading the charge.

## The Importance of Common Data Schemas

The main reason I've worked on these data formats is to make data more accessible, and hopefully make publishing easier and cheaper for data publishers. Another word often used with geospatial standards is 'interoperability' &ndash; the ability for different systems to share information. Common data formats are part of interoperability, but I'd argue the much more important thing to enable real interoperability is common 'data schemas'. This just means that the names of the data attributes and the potential values in them are all the same. Today if you were to download parcel data from 5 different adjacent counties in California the chances are they would all have different data schemas. And so if you wanted to combine the data into a single dataset containing all the parcels in the Bay Area you'd need to understand the meaning of each of their attributes and figure out how to transform them to all be the same.

Since parcel data is quite valuable there are a number of companies who make it their business to acquire parcel data from official sources and transform it into a common data schema. At some level this makes sense, but at another level it's crazy! It'd be much better if every parcel data provider made their data available in the same way. And while there are companies that will clean up and resell parcel data, most of the datasets provided by governments don't have someone aligning data. <todo &ndash; find common open data portal datasets and make some examples>.

There have been a number of noble attempts to try to build interoperability at the data schema level. I'll likely do a post just on my view of the history and what went wrong. But I believe it's worth us taking another shot at this, and I believe cloud-native geospatial should help make it easier. It is foundational to making any type of Spatial Data Infrastructure effective and useful &ndash; just downloading a bunch of incompatible data and needing to spend hours and days just to treat it as one dataset is far from the accessibility we dream of. Indeed, that's basically the state of numerous open data portals today &ndash; yes, you can access the data, but it's not actually interoperable with data from other portals. And I would argue it's not 'infrastructure' &ndash; it's cool, but it's not a reliable foundation you can depend on.

The potential of common data schemas is that you could have many diverse entities collaborating towards a common purpose &ndash; organizing the world's information so that it's more useful and accessible. Once a common data schema is established than everyone publishing data in that format is contributing towards a collective understanding of our world. That sounds grand, but what I'm talking about is pretty basic things &ndash; if a dataset of buildings has attributes for the number of floors and how tall it is then it would use `numfloors` and `height` as the name of the attributes, and they'd follow the same conventions.

To be clear, I don't see a single data schema that everyone has to align to. I think there's a way to have a small, common core of information that gives data providers the flexibility to use the pieces that are relevant to them and easily add their own. I'll go into details of how I think this could work, but it's mostly inspired by how we built the SpatioTemporal Asset Catalog core and extensions.

I am aware that there's a whole field around the 'semantic web', that attempts to solve this problem generically, making it so you can convey the 'meaning' of your data in a structured way. I love the vision, but I've not seen it work in reality, and to me the practical thing is just to focus on common data schemas that have agreed upon meaning. And really to just pick the most common ones, where users most acutely feel the interoperability problem.

The goal for all of this for me is not just to make life a bit easier for the user who wants to combine a few sets of data. The potential is that it'll enable new types of software and new innovation on specific types of data. The end goal to me is really to enable better decision making, and for that we need software that brings together diverse data and runs models, makes predictions, and lets people try out different scenarios. The data should be fully abstracted, but anyone using some cool new software should be able to just point at the right data sources and have it all 'just work', instead of needing to be a data engineer. And I believe that if we get the interoperable data infrastructure right then it'll become a foundation for much more innovative software. 

The other clear angle for all of this is to be able to better power AI-based interfaces, and to realize the vision for 'Queryable Earth'. 

{{< img src="images/20240222-ids-1.png" alt="" caption="" >}}

This query is quite easy with a GIS, after downloading the [building footprints dataset](https://data.sfgov.org/Geographic-Locations-and-Boundaries/Building-Footprints/ynuv-fyni) from [data.sfgov.org](https://data.sfgov.org/).

But we should be able to ask more challenging questions, like 'How many buildings are there in San Francisco over 200 feet tall'? Or even 'what percent of buildings in San Francisco are within 500 feet of a bus stop?'. These are questions that a standard data schema makes possible and potentially even easy. The first question just needs standardization of 'height' in any building data set, the second needs standardization of 'bus stop' datasets. 

And though the data exists on the SF data portal today, it is not available in ChatGPT. I believe that if we are able to standardize datasets and have lots of information in those standard formats then they'd show up in LLM's much more easily. Indeed it'd be almost trivial to write a GPT that would work with any standard 'building' dataset, especially if the standardization included a data schema that explained exactly what each field meant.

{{< img src="images/20240222-ids-2.png" alt="" caption="" >}}

The above is a GPT that I made with the San Francisco dataset illustrates the point. I was able to get an answer to the number of buildings over 200 feet tall in San Francisco. But it's not actually right. The attribute it used for ft looks like it was actually a shortened name for `P2010mass_ZmaxN88ft` which is defined in the pdf as 'Input building mass (of 2010,) maximum Z vertex elevation, NAVD 1988 ft', so it's the mass of the building, calculated by LiDAR. There are actually at least 11 potential height values in this dataset. I can instruct the GPT which is the right one, so anyone in the future could get the right results.

But then if you wanted to add building footprint datasets from different cities you'd have to instruct the GPT on how to handle each one of them. It'd be much better if we just settled on a small number of attributes for everyone add in to their dataset that are known. Decide that a `height` attribute in a building dataset is always denominated in meters, that `numfloors` is a number specifying the number of floors, and then it'd be much easier for AI to make use of it. Making a 'building data GPT' would enable anyone to upload a building dataset that follows the common data schema, and it'd be easy to instruct it to make sure it does the analysis right &ndash; you could just tell it that height is specified in meters, and it should translate other values appropriately. It would also then start to become common knowledge that a `height` field is meters, so that the general Large Language Models would start to understand any building data and know what to do with it.

This general pattern should work for any common type of data that we can create data schemas. Parcels, Pollution, Trees, Cell Phone Coverage, Parking Meters, Oil Wells, Demographics, Police Incidents, Streams, etc. I'd love to see Field Boundaries for agriculture, and then common attributes for NDVI, Leaf Area Index, Soil Water Content, Yield predictions, Crop Type and more. I'll hold off here on ideas on how to go about achieving this, but I believe it is possible that we could start to get real collaboration around data schemas and the data itself. But the main point I aim to make here is that common data schemas have huge potential to make everyone's lives easier and to spur more collaboration, and are worth trying to collaborate on.

## The importance of Common IDs

The other related topic is global ID's. You could easily have a common data schema without doing anything to ensure that ID's are globally unique, and it'd be quite valuable. But I believe that ID's can enable another level of goodness, in particular on foundational datasets. One of those data sets is the field boundaries from above. I recently learned about an awesome organization called [Varda](https://varda.ag) that is taking this challenge head on. You should read their website about how stable, global field ID's enable transparency, regenerative ag practices, and supply chain traceability. I agree with all of those, but I'm most interested in how it enables better data collaboration, in ways that are applicable to any type of foundational geospatial dataset. 

Key to this is the ability for the id to serve as a 'join key'. This means that you can have a number of different datasets that all provide information about a specific type of data (like a farmer's field or a building), and having a common id makes it incredibly easy to combine those into a single dataset. This opens up a possibility to distribute information that's spatial in nature, but not have to always include a geometry. At Planet there are a number of [Planetary Variables](https://www.planet.com/products/planetary-variables/), like crop biomass and soil water content, that are updated daily. These are rasters, and many workflows involve downloading the full raster every day. But they are easily summarized into a field boundary. If there were global field ID's then delivery of soil water content could just be a CSV or Parquet file with two fields &ndash; the daily summary and the ID. There would be no need to pass the geometries back and forth every time, the user could just update their table by joining their local geometries with the soil water content for the day by using the ID. 

Having global ID's in turn can enable others to base their work on top of those ID's. In the field boundaries example there would be no need for everyone to make a new field boundary every time they wanted to create some insight about the field. Someone could just get really good at yield predictions for corn, and they'd use the global boundaries as input to their process, and could output the predicted yield per field by just specifying the ID. Or in the buildings example someone focused on [exposure data for disaster risk assessment](https://docs.riskdatalibrary.org/en/latest/reference/schema/#exposure-metadata) could leverage the ID's of the best global dataset and just focus on adding the building material and occupants attributes. Full communities could form just adding a particular set of attributes to globally defined datasets, for example the [GeoAsset Project](https://www.cgfi.ac.uk/spatial-finance-initiative/geoasset-project/) could just be people collaborating globally to define ownership information on any building or field boundary.

## IDs, Schemas, and Cloud-Native Spatial Data Infrastructure

So obviously the two topics are pretty closely related, and I think together they point towards a new type of global collaboration around geospatial data. Thus far the major collaborative geospatial data effort has been OpenStreetMap, which is very much built in the model of Wikipedia. Just like Wikipedia aims to one huge encyclopedia so too OSM aims to be the one place to put all your map data. 

But I've long had a hunch that a better model for geospatial data is the open source software ecosystem. All of open source software isn't in one big repository with one set of rules governing one community. There's many different repositories, and each has their own way of doing things, with different values embedded into their approach. But it's not totally decentralized &ndash; there are some large projects with many collaborators. And there are also a number of foundational projects that everyone depends on, some large, some small but critical. I believe a 'cloud native spatial data infrastructure' take a similar structure of a number of related communities that are cross-dependent with one another to varying degrees. But they'll share a common ethos of collaboration, working together towards a collective problem. It will center around global datasets, establishing core ids & geometries that everyone relies upon, but flexible schemas will enable different communities to build on top of that core. 

There's many lifetimes of work to figure out the best ways to collaborate, and I believe there will be lots of value in many different groups trying out a number of approaches. But my hope is that we can establish common standards at the core for formats, schemas & ID's, and tools that make it easy to use those, so that anyone making a new data standard isn't starting from scratch.

## What's next
I've got a number of posts to write on this subject, diving deeper into both data schemas and ID's in their own posts, and going into what we can learn from the last major push into data schemas for geospatial. I hope to highlight a few positive examples of successful data schemas, like [GTFS](https://gtfs.org/) and [GBFS](https://gbfs.org/). If you know of any good examples, particularly in the geospatial world, but even in other domains, I'd love to hear about them. I'm also very interested in tools that help define and validate data schemas in the modern mainstream data world of BigQuery & Snowflake. And I'm really interested to start trying to take on specific types of geospatial data and see if we can help establish some common, flexible data schemas that enable better interoperability.
